<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <title>Title</title>
</head>
<body>

</body>
</html>



1.创建项目和爬虫：
>1.创建项目：“scrapy startproject [爬虫的名字]”
>2.创建爬虫：进入到项目所在的路径，执行命令：“scrapy genspider [爬虫名字]” "爬虫域名"，爬虫名字不能和项目名称一致。


2.项目目录结构
>1.items.py:用来存放爬虫爬下来的数据模型
>2.middlewares.py：用来存放各种中间件文件
>3.pipelines.py：用来将items的模型存储到本地磁盘中
>4.settings.py：本爬虫的一些配置信息（比如请求头、多久发送一次请求、ip代理池等）
>5.scrapy.cfg： 项目的配置文件
>6.spider包：以后所有的爬虫，都存放到这个里面


3.糗事百科scrapy爬虫笔记：
>1.response是一个scrapy.http.response.html import HtmlResponse对象，可以执行xpath和css语法提取数据
>2.提取出来的数据是一个Selector或者是一个SelectorList对象，如果想要获取其中的字符串，那么应该执行getall或者get方法
>3.getall方法：获取Selector中的所有文本，返回是一个列表
>4.get方法：获取是Selector中的第一个文本，返回是一个str类型
>5.如果结束解析回来要传给pipline处理，那么可以使用yield来返回。或者搜集所有的item，最后统一使用return返回
>6.item： 建议再items.py中定义好模型，以后就不要使用字典
>7.pipeline这个是专门用来保存数据，其中有三个方法是经常会使用的
    * open_spider(self,spider):当爬虫被打开时候执行
    * process_item(self, item, spider):当爬虫有item传过来的时候会被调用
    * close_spider(self,spider):当爬虫关闭时会被调用
    要激活pipeline，应该在settings.py中设置ITEM_PIPELINES


4.JsonLinesItemExporter和JsonItemExporter：
保存json数据的时候，可以使用这两个类，让操作变得更加简单，
1.JsonItemExporter：这个是每次都把数据添加到内存转你个，最后统一写入到磁盘中。好处是存储的数据是一个满足json规则的数据，坏处是如果数据量比较大那么比较耗内存
2.JsonLinesItemExporter：这个是每次调用export_item的时候就把这个item存储到硬盘中，坏处是没一个字典是一行，整个文件不是一个满足json格式的文件。好处是每次处理数据的时候就直接存储到了硬盘中，这样不会耗内存，数据也比较安全